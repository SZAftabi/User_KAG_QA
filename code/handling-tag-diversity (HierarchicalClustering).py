# -*- coding: utf-8 -*-
"""Hnadling-Tag-Diversity (Libraries).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JAsqIxKRWMw2WAAYy2UQLkqqlnkiNmQn
"""

def hierarchical_clustering(embeddings, n_clusters, metric, dist_threshold, linkage, full_tree):
    agglomerative_cluster = AgglomerativeClustering(
        n_clusters=None,
        metric=metric,
        distance_threshold=dist_threshold,
        linkage=linkage,
        compute_full_tree = full_tree
    )
    pr = agglomerative_cluster.fit_predict(embeddings)
    model = agglomerative_cluster.fit(embeddings)
    return pr, model

def extract_cluster_statistics(embeddings_2d, cluster_labels, tags, tag_frequency_dict):
    data = pd.DataFrame(columns=['X', 'Y', 'Cluster', 'Tag'])
    cluster_means, cluster_variances, cluster_sizes = [], [], []
    representative_samples, representative_samples_freq = {}, {}
    representative_samples_freq_2nd = {}
    representative_samples_freq_3nd = {}

    for cluster_label in np.unique(cluster_labels):
        samples_tags = [tags[i]
                        for i, lbl in enumerate(cluster_labels)
                        if lbl == cluster_label]
        indexes = [i
                   for i, lbl in enumerate(cluster_labels)
                   if lbl == cluster_label]
        samples_2d = embeddings_2d[cluster_labels == cluster_label]

        cluster_center = np.mean(samples_2d, axis=0)
        distances = cdist(samples_2d, [cluster_center])
        nearest_sample_idx = np.argmin(distances)
        representative_samples[cluster_label] = samples_tags[nearest_sample_idx]
        print(samples_tags)

        representative_samples_freq[cluster_label] = max(
            samples_tags,
            key=lambda tag: tag_frequency_dict.get(tag, 0))
        mostfreq_sample_idx = samples_tags.index(representative_samples_freq[cluster_label])


        samples_tags_copy = copy.deepcopy(samples_tags)
        samples_tags_copy.remove(representative_samples_freq[cluster_label])
        representative_samples_freq_2nd[cluster_label] = max(
            samples_tags_copy,
            key=lambda tag: tag_frequency_dict.get(tag, 0))
        mostfreq_sample_idx_2nd = samples_tags.index(representative_samples_freq_2nd[cluster_label])


        samples_tags_copy.remove(representative_samples_freq_2nd[cluster_label])
        representative_samples_freq_3nd[cluster_label] = max(
            samples_tags_copy,
            key=lambda tag: tag_frequency_dict.get(tag, 0))
        mostfreq_sample_idx_3nd = samples_tags.index(representative_samples_freq_3nd[cluster_label])


        distances_to_representative = cdist(samples_2d, [samples_2d[nearest_sample_idx]])
        mean_distance = np.mean(distances_to_representative)
        variance_distance = np.var(distances_to_representative)
        cluster_size = len(samples_tags)
        cluster_means.append(mean_distance)
        cluster_variances.append(variance_distance)
        cluster_sizes.append(cluster_size)

        cluster_data = pd.DataFrame(
            {
                'X': samples_2d[:, 0],
                'Y': samples_2d[:, 1],
                'Cluster': samples_tags[mostfreq_sample_idx],
                'Tag': samples_tags,
                'Indexes': indexes
            }
        )
        data = pd.concat([data, cluster_data])


    cluster_stats = pd.DataFrame({
        'Cluster': np.unique(cluster_labels),
        'Representative sample': [representative_samples[cluster] for cluster in np.unique(cluster_labels)],
        'representative_samples_freq': [representative_samples_freq[cluster] for cluster in np.unique(cluster_labels)],
        'representative_samples_freq_2nd': [representative_samples_freq_2nd[cluster] for cluster in np.unique(cluster_labels)],
        'representative_samples_freq_3nd': [representative_samples_freq_3nd[cluster] for cluster in np.unique(cluster_labels)],
        'Mean_Distance': cluster_means,
        'Variance_Distance': cluster_variances,
        'Cluster_Size': cluster_sizes,
    }).set_index('Cluster', drop=True)


    silhouette_metric = silhouette_score(
        embeddings_2d,
        cluster_labels
        )
    calinski_harabasz_metric = calinski_harabasz_score(
        embeddings_2d,
        cluster_labels
        )
    davies_bouldin_metric = davies_bouldin_score(
        embeddings_2d,
        cluster_labels
        )
    return cluster_stats, silhouette_metric, calinski_harabasz_metric, davies_bouldin_metric, data, len(np.unique(cluster_labels))

def plot_dendrogram(model, **kwargs):
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    dendrogram(linkage_matrix, **kwargs)

    return linkage_matrix

n_clusters=None
metric= 'euclidean'
linkage_= 'ward'
full_tree = True

_, model_Agg = hierarchical_clustering(
        embeddings = node_embeddings,
        n_clusters = n_clusters,
        metric = metric,
        dist_threshold = 0,
        linkage = linkage_,
        full_tree = full_tree
    )

plt.figure(figsize=(10, 8))
linkage_matrix = plot_dendrogram(model_Agg)
plt.xticks([])
plt.xlabel('')
plt.savefig(f"PATH/TO/SAVE/Dendogram.png", dpi=300)
plt.show()

pd.options.display.max_rows = 200
thresholds = [32, 16, 8]
all_cluster_labels, representative_samples, Whole_data = [], {}, []


for level, threshold in enumerate(thresholds):
    print(f"Threshold: {threshold}")
    cluster_labels = fcluster(
        linkage_matrix,
        t=threshold,
        criterion='distance'
        )
    all_cluster_labels.append(cluster_labels)

    cluster_stats, silhouette_metric,\
     calinski_harabasz_metric, davies_bouldin_metric,\
      data, num_classes = extract_cluster_statistics(
        embeddings_2d=embeddings_2d,
        cluster_labels=cluster_labels,
        tags=list(G.nodes()),
        tag_frequency_dict=tag_frequency_dict
        )

    Whole_data.append(data)
    for c_id, rep_name in cluster_stats['representative_samples_freq'].items():
        if level == 1:
          if any(
              value == rep_name
              for (key_level, _), value in representative_samples.items()
              if key_level == level - 1
          ):
              representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_2nd'][c_id]
          else:
              representative_samples[(level, c_id)] = rep_name
        elif level == 2:
            if any(
                value == rep_name
                for (key_level, _), value in representative_samples.items()
                if key_level == level - 1
            ):
                representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]
            elif any(
                value == rep_name
                for (key_level, _), value in representative_samples.items()
                if key_level == level - 2
            ):
                representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]
            else:
                representative_samples[(level, c_id)] = rep_name
        else:
            representative_samples[(level, c_id)] = rep_name


    print("Cluster Statistics:")
    display(cluster_stats)
    print("Silhouette Metric:", silhouette_metric)
    print("Calinski Harabasz Metric:", calinski_harabasz_metric)
    print("Davies Bouldin Metric:", davies_bouldin_metric)

hierarchical_data = []
for tag_idx, tag in enumerate(list(G.nodes())):
    level_1_cluster = representative_samples.get((0, all_cluster_labels[0][tag_idx]))
    level_2_cluster = representative_samples.get((1, all_cluster_labels[1][tag_idx]))
    level_3_cluster = representative_samples.get((2, all_cluster_labels[2][tag_idx]))
    hierarchical_data.append([level_1_cluster, level_2_cluster, level_3_cluster, tag])

hierarchical_df = pd.DataFrame(
    hierarchical_data,
    columns=["Level 1", "Level 2", "Level 3", "Tag"]
    )